{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "inference_imdb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "GW06HvkiKY9Q",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "0f1e7412-f6bb-4d64-8dae-682beed0764d"
      },
      "source": [
        "import torch\n",
        "import torchtext\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import spacy\n",
        "import json\n",
        "from string import punctuation\n",
        "\n",
        "#ENTER THE PATH WHERE YOU HAVE SAVED MODEL\n",
        "MODEL_PATH = \"/content/drive/My Drive/Colab Notebooks/movieReccomendation/\"\n",
        "VOCAB_PATH = \"/content/drive/My Drive/Colab Notebooks/movieReccomendation/vocab/vocab.json\"\n",
        "\n",
        "sp = spacy.load('en_core_web_sm')\n",
        "all_stopwords = sp.Defaults.stop_words\n",
        "\n",
        "def load(file_path):\n",
        "    with open(file_path, 'r', encoding='utf8') as f:\n",
        "        entry = json.load(f)\n",
        "    return entry['word2id']\n",
        "dictionary = load(VOCAB_PATH)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "class SentimentClassification(nn.Module):\n",
        "    def __init__(self, input_dim, embedding_dim, hidden_dim, output_dim, n_layers, pad_idx, dropout):\n",
        "        super(SentimentClassification, self).__init__()\n",
        "\n",
        "        self.output_dim = output_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.n_layers = n_layers\n",
        "\n",
        "        self.embedding.weight.requires_grad = False\n",
        "        self.lstm = nn.LSTM(embedding_dim, \n",
        "                           hidden_dim, \n",
        "                           num_layers=n_layers, \n",
        "                           bidirectional=True, \n",
        "                           dropout=dropout, batch_first=True)\n",
        "        \n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        self.fc = nn.Linear(hidden_dim * 2, output_dim)\n",
        "        \n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.sig = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, text, text_lengths):\n",
        "        batch_size = text.size(1)       \n",
        "        #text_dim = (seq_len, batch_size)\n",
        "        \n",
        "        #embed_dim = (seq_len, batch_size, embed_dim)\n",
        "        embeds = self.embedding(text)\n",
        "        \n",
        "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embeds, text_lengths, enforce_sorted=False)\n",
        "        packed_output, (hidden, cell) = self.lstm(packed_embedded)\n",
        "\n",
        "        #unpack sequence\n",
        "        output, output_lengths = nn.utils.rnn.pad_packed_sequence(packed_output)\n",
        "        #print('out dim ', output.size())\n",
        "        #output = [sent len, batch size, hid dim * num directions]\n",
        "        # stack up lstm outputs\n",
        "        \n",
        "        #hidden = [num layers * num directions, batch size, hid dim]\n",
        "        #cell = [num layers * num directions, batch size, hid dim] \n",
        "        \n",
        "        #concat the final forward (hidden[-2,:,:]) and backward (hidden[-1,:,:]) hidden layers\n",
        "        #and apply dropout\n",
        "        \n",
        "        hidden = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1))\n",
        "        out = self.fc(hidden)\n",
        "        #hidden = [batch size, hid dim * num directions] \n",
        "        return out\n",
        "\n",
        "    def init_hidden(self, batch_size):\n",
        "        ''' Initializes hidden state '''\n",
        "        # Create two new tensors with sizes n_layers x batch_size x hidden_dim,\n",
        "        # initialized to zero, for hidden state and cell state of LSTM\n",
        "        weight = next(self.parameters()).data\n",
        "        \n",
        "        if (torch.cuda.is_available()):\n",
        "            hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda(),\n",
        "                  weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda())\n",
        "        else:\n",
        "            hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_(),\n",
        "                      weight.new(self.n_layers, batch_size, self.hidden_dim).zero_())\n",
        "        \n",
        "        return hidden\n",
        "\n",
        "\n",
        "nlp = spacy.load('en_core_web_sm', disable = ['ner', 'parser', 'textcat', 'tagger', '...'])\n",
        "model_ = torch.load(MODEL_PATH+'SentimentClf_model3.pt', map_location=device)\n",
        "model_.eval()\n",
        "\n",
        "\n",
        "def token_filter(token):\n",
        "    \"\"\"Filter the token for text_preprocessing function.\n",
        "    Check if the token is not: punctuation, whitespace, stopword or digit.\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    token : spacy.Token\n",
        "        Token passed from text_preprocessing function.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    Bool\n",
        "       True if token meets the criteria, otherwise False.\n",
        "        \n",
        "    \"\"\"\n",
        "    return not (token.is_punct | token.is_space | token.is_digit | token.like_num | token.is_stop)\n",
        "\n",
        "def spacy_preprocess(review):\n",
        "    review = review.lower()\n",
        "    print(review)\n",
        "    text = [word for word in nlp(review)]\n",
        "    # Remove stopwords, spaces, punctutations and digits\n",
        "    text = [word for word in text if token_filter(word)]\n",
        "    print(text)\n",
        "        # Lemmatization\n",
        "    text = [token.lemma_ for token in text if token.lemma_ != '-PRON-']\n",
        "    return text\n",
        "\n",
        "def predict_sentiment(model_, sentence):\n",
        "    model_.eval()\n",
        "    #tokenized = [tok.text for tok in nlp.tokenizer(preprocess(sentence))]\n",
        "    tokenized = spacy_preprocess(sentence)\n",
        "    print(tokenized)\n",
        "    tokens_without_sw= [word for word in tokenized if not word in all_stopwords]\n",
        "    indexed = [dictionary[t] for t in tokens_without_sw]\n",
        "    #print(indexed)\n",
        "    length = [len(indexed)]\n",
        "    tensor = torch.LongTensor(indexed).to(device)\n",
        "    tensor = tensor.unsqueeze(1)\n",
        "    length_tensor = torch.LongTensor(length)\n",
        "    prediction = torch.sigmoid(model_(tensor, length_tensor))\n",
        "    return prediction.item()\n",
        "\n",
        "def inference(review:str):\n",
        "    pred_prob = predict_sentiment(model_, review)\n",
        "    print(pred_prob)\n",
        "    if pred_prob>=0.5:\n",
        "        return 'positive'\n",
        "    else:\n",
        "        return 'negative'\n",
        "\n",
        "review = input(\"ENTER THE REVIEW HERE: \")\n",
        "#review = \"it is worst movie with really bad characters.\"\n",
        "sentiment = inference(review)\n",
        "print(sentiment)"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ENTER THE REVIEW HERE: it is worst movie with really bad characters.\n",
            "it is worst movie with really bad characters.\n",
            "[worst, movie, bad, characters]\n",
            "['wrong', 'movie', 'bad', 'character']\n",
            "0.006410098634660244\n",
            "negative\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}