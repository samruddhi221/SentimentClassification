{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "IMDB_classification.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y_BRSCHq5e2H",
        "colab_type": "code",
        "outputId": "2295f1cd-e118-4f98-97e7-537272bdc2c3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "import torch.nn\n",
        "import torchtext\n",
        "from torchtext import data\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import timeit\n",
        "\n",
        "import spacy\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "se0-PrA06KnQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "PATH = \"/content/drive/My Drive/Colab Notebooks/movieReccomendation/dataset/final_data/\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2G86fNh6eCFJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "a4aee710-f11e-4a05-b9ec-94fdefe92fae"
      },
      "source": [
        "X = pd.read_csv(PATH+'train.csv')\n",
        "print(X.head())"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "   sentiment                                       clean_review\n",
            "0          0  history year movie critic lead general public ...\n",
            "1          0  love movie admit well straight video movie see...\n",
            "2          1  think movie laugh anne ramsey play unforgettab...\n",
            "3          1  teen set camp oregon wilderness despite warn p...\n",
            "4          1  affable aspire cartoonist hoop mccann wonderfu...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y8yofGqH6zqr",
        "colab_type": "code",
        "outputId": "b99b7f81-4b97-4206-8cd6-79ef23235892",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "def load_file(filepath, device, MAX_VOCAB_SIZE = 40000):\n",
        "    tokenizer = lambda x: str(x).split()\n",
        "\n",
        "    TEXT = data.Field(sequential=True,tokenize=tokenizer,fix_length=100)\n",
        "    LABEL = data.Field(sequential=False, use_vocab=False)\n",
        "    tv_datafields = [('sentiment', LABEL), ('clean_review', TEXT)]\n",
        "    # Step two construction our dataset.\n",
        "    train, valid, test = data.TabularDataset.splits(path=filepath,\n",
        "                                                    train='train.csv', validation='val.csv',\n",
        "                                                    test='test.csv', format=\"csv\",\n",
        "                                                    skip_header=True, fields=tv_datafields)\n",
        "    print(train[1].__dict__.keys())\n",
        "    # Step three We should build_vocab for the field with use_vocab=True. \n",
        "    # If not we will get an error during the loop section.\n",
        "    TEXT.build_vocab(train, max_size = MAX_VOCAB_SIZE)\n",
        "    \n",
        "    print(\"build vocab success...\")\n",
        "    \n",
        "    # Step four construct our iterator to our dataset. \n",
        "    train_iter = data.BucketIterator(train, device=device, batch_size=32, sort_key=lambda x: len(x.text),\n",
        "                                     sort_within_batch=False, repeat=False)\n",
        "    valid_iter = data.BucketIterator(valid, device=device, batch_size=32, sort_key=lambda x: len(x.text),\n",
        "                                     sort_within_batch=False, repeat=False)\n",
        "    test_iter = data.BucketIterator(test, device=device, batch_size=32, sort_key=lambda x: len(x.text),\n",
        "                                     sort_within_batch=False, repeat=False)\n",
        "    print(\"construct iterator success...\")\n",
        "    return TEXT, LABEL, train, valid, test, train_iter, valid_iter, test_iter\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "TEXT, LABEL, train, valid, test, train_iter, valid_iter, test_iter = load_file(PATH, device)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dict_keys(['sentiment', 'clean_review'])\n",
            "build vocab success...\n",
            "construct iterator success...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zS7Tlb5ZbMbH",
        "colab_type": "code",
        "outputId": "5eba97e1-7ed1-4014-da8c-3e9534799224",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "# most common words and their frequencies.\n",
        "print(TEXT.vocab.freqs.most_common(20))\n",
        "\n",
        "# top ten index to words transform.\n",
        "print(TEXT.vocab.itos[:10])"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('movie', 80774), ('film', 74742), ('like', 34849), ('time', 24701), ('good', 23395), ('character', 22260), ('watch', 21587), ('story', 19637), ('see', 19625), ('think', 19329), ('well', 19018), ('scene', 16640), ('great', 15855), ('look', 15711), ('know', 15233), ('end', 14575), ('bad', 14451), ('people', 14447), ('go', 14312), ('get', 13922)]\n",
            "['<unk>', '<pad>', 'movie', 'film', 'like', 'time', 'good', 'character', 'watch', 'story']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6wvWw8cjWZ4s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SentimentClassification(nn.Module):\n",
        "    def __init__(self, input_dim, embedding_dim, hidden_dim, output_dim):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(input_dim, embedding_dim)\n",
        "        self.rnn = nn.RNN(embedding_dim, hidden_dim)\n",
        "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "        \n",
        "    def forward(self, text):\n",
        "        # text [sentence length, batch_size]\n",
        "\n",
        "        embedded = self.embedding(text)\n",
        "        \n",
        "        # embedded = [sentence length, batch_size, emb dim]\n",
        "        output, hidden = self.rnn(embedded)\n",
        "        \n",
        "        # output = [sent len, batch_size, hid dim]\n",
        "        # hidden = [1, batch_size, hid dim]\n",
        "        assert torch.equal(output[-1,:,:], hidden.squeeze(0))\n",
        "        \n",
        "        return self.fc(hidden.squeeze(0))   "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IMmgqA4mXTw7",
        "colab_type": "code",
        "outputId": "10a058f9-f1ae-4593-dba0-850d6131d37e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "INPUT_DIM = len(TEXT.vocab)\n",
        "print(INPUT_DIM)\n",
        "EMBEDDING_DIM = 400\n",
        "HIDDEN_DIM = 256\n",
        "OUTPUT_DIM = 1\n",
        "\n",
        "model = SentimentClassification(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "40002\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qvHoR3wLXbPA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimizer = optim.SGD(model.parameters(), lr=1e-3)\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "model = model.to(device)\n",
        "criterion = criterion.to(device)\n",
        "\n",
        "def calculateAccuracy(preds, y):\n",
        "    '''\n",
        "    Return accuracy per batch ..\n",
        "    '''\n",
        "    # round predictions to the closest integer\n",
        "    rounded_preds = torch.round(torch.sigmoid(preds))\n",
        "    correct = (rounded_preds == y).float()\n",
        "    acc = correct.sum() / len(correct)\n",
        "    \n",
        "    return acc"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9_jp7Gi_e3gA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(model, iterator, optimizer, criterion):\n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "    \n",
        "    model.train()\n",
        "    \n",
        "    for i, batch in enumerate(iterator):\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        predictions = model(batch.clean_review).squeeze(1)\n",
        "        \n",
        "        # note we must transform the batch.label into float or we will get an error later.\n",
        "        loss = criterion(predictions, batch.sentiment.float())\n",
        "        acc = calculateAccuracy(predictions, batch.sentiment)\n",
        "        \n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        epoch_loss += loss.item()\n",
        "        epoch_acc += acc.item()\n",
        "        if i % 200 == 199:\n",
        "            print(f\"[{i}/{len(iterator)}] : epoch_acc: {epoch_acc / len(iterator):.2f}\")\n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sAHJP0dbfZ9b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate(model, iterator, criterion):\n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "    \n",
        "    model.eval()\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for i, batch in enumerate(iterator):\n",
        "            # prediction [batch_size]\n",
        "            predictions = model(batch.clean_review).squeeze(1)\n",
        "            \n",
        "            loss = criterion(predictions, batch.sentiment.float())\n",
        "            \n",
        "            acc = calculateAccuracy(predictions, batch.sentiment)\n",
        "        \n",
        "            epoch_loss += loss.item()\n",
        "            epoch_acc += acc.item()\n",
        "            \n",
        "    return epoch_loss / len(iterator),  epoch_acc / len(iterator)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KFOcVzwhf1_e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import time\n",
        "\n",
        "def epoch_time(start_time, end_time):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time  / 60)\n",
        "    elapsed_secs = int(elapsed_time -  (elapsed_mins * 60))\n",
        "    return  elapsed_mins, elapsed_secs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tu50NiCef6Ex",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 782
        },
        "outputId": "c7941277-cd5e-4443-dbcc-5cfc06b26400"
      },
      "source": [
        "N_epoches = 5\n",
        "\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "for epoch in range(N_epoches):\n",
        "    \n",
        "    start_time = time.time()\n",
        "    \n",
        "    train_loss, train_acc = train(model, train_iter, optimizer, criterion)\n",
        "    valid_loss, valid_acc = evaluate(model, valid_iter, criterion)\n",
        "    \n",
        "    end_time = time.time()\n",
        "    \n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "    \n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(model.state_dict(), 'Sentiment-model.pt')\n",
        "        \n",
        "    print(f'Epoch:  {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
        "    print(f'\\tTrain  Loss: {train_loss: .3f} | Train Acc: {train_acc*100:.2f}%')\n",
        "    print(f'\\tValid  Loss: {valid_loss: .3f} | Valid Acc: {valid_acc*100:.2f}%')"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[199/1250] : epoch_acc: 0.08\n",
            "[399/1250] : epoch_acc: 0.16\n",
            "[599/1250] : epoch_acc: 0.24\n",
            "[799/1250] : epoch_acc: 0.32\n",
            "[999/1250] : epoch_acc: 0.40\n",
            "[1199/1250] : epoch_acc: 0.48\n",
            "Epoch:  01 | Epoch Time: 14m 11s\n",
            "\tTrain  Loss:  0.698 | Train Acc: 49.78%\n",
            "\tValid  Loss:  0.698 | Valid Acc: 48.35%\n",
            "[199/1250] : epoch_acc: 0.08\n",
            "[399/1250] : epoch_acc: 0.16\n",
            "[599/1250] : epoch_acc: 0.24\n",
            "[799/1250] : epoch_acc: 0.32\n",
            "[999/1250] : epoch_acc: 0.40\n",
            "[1199/1250] : epoch_acc: 0.48\n",
            "Epoch:  02 | Epoch Time: 14m 14s\n",
            "\tTrain  Loss:  0.696 | Train Acc: 50.08%\n",
            "\tValid  Loss:  0.697 | Valid Acc: 50.40%\n",
            "[199/1250] : epoch_acc: 0.08\n",
            "[399/1250] : epoch_acc: 0.16\n",
            "[599/1250] : epoch_acc: 0.24\n",
            "[799/1250] : epoch_acc: 0.32\n",
            "[999/1250] : epoch_acc: 0.40\n",
            "[1199/1250] : epoch_acc: 0.48\n",
            "Epoch:  03 | Epoch Time: 14m 19s\n",
            "\tTrain  Loss:  0.695 | Train Acc: 50.16%\n",
            "\tValid  Loss:  0.697 | Valid Acc: 50.42%\n",
            "[199/1250] : epoch_acc: 0.08\n",
            "[399/1250] : epoch_acc: 0.16\n",
            "[599/1250] : epoch_acc: 0.24\n",
            "[799/1250] : epoch_acc: 0.32\n",
            "[999/1250] : epoch_acc: 0.41\n",
            "[1199/1250] : epoch_acc: 0.49\n",
            "Epoch:  04 | Epoch Time: 14m 23s\n",
            "\tTrain  Loss:  0.695 | Train Acc: 50.92%\n",
            "\tValid  Loss:  0.696 | Valid Acc: 50.60%\n",
            "[199/1250] : epoch_acc: 0.08\n",
            "[399/1250] : epoch_acc: 0.16\n",
            "[599/1250] : epoch_acc: 0.24\n",
            "[799/1250] : epoch_acc: 0.32\n",
            "[999/1250] : epoch_acc: 0.40\n",
            "[1199/1250] : epoch_acc: 0.48\n",
            "Epoch:  05 | Epoch Time: 14m 25s\n",
            "\tTrain  Loss:  0.694 | Train Acc: 50.45%\n",
            "\tValid  Loss:  0.697 | Valid Acc: 48.49%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Plnn8k2Ko4Fo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "2b3831e9-8fb0-4c20-c864-745b6142df3d"
      },
      "source": [
        "model.load_state_dict(torch.load('Sentiment-model.pt'))\n",
        "\n",
        "test_loss, test_acc = evaluate(model, test_iter, criterion)\n",
        "\n",
        "print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test Loss: 0.695 | Test Acc: 52.07%\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}